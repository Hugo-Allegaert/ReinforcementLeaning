{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c4283e",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42db6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame\n",
    "import random\n",
    "from io import StringIO ## for Python 3\n",
    "from gym import Env, spaces, utils\n",
    "#from gym.envs.toy_text.utils import categorical_sample\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875c803",
   "metadata": {},
   "source": [
    "### Init game env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed89fe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: 500\n",
      "Action: 6\n"
     ]
    }
   ],
   "source": [
    "env_name = \"Taxi-v3\"\n",
    "env = gym.make(env_name)\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "print('Observation:', env.observation_space.n)\n",
    "print('Action:', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ccc2ae",
   "metadata": {},
   "source": [
    "### Init Q learning params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492f45b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Table: [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "num_episode = 1000\n",
    "max_step = 100\n",
    "\n",
    "# How quickly abandon the previous q value in the table for the new q value for the same pair (s,a) at a later timestep\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99 # gamma\n",
    "\n",
    "# prob to exploring over exploit env (greedy) : if r(0, 1) > exploration_rate = exploit else explore\n",
    "exploration_rate = 1\n",
    "max_exploration = 1\n",
    "min_exploration = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "# Calculate the new q value\n",
    "# Qnew(s,a) = (1 - lr) * Qold(s,a) + (lr * Qlearned(s,a))\n",
    "# Qlearned = (ResAction + (dr * maxQt+1([s, ...a])))\n",
    "\n",
    "print ('Q Table:', q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6f55c",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09147292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    def __init__(self, env):\n",
    "        self.action_space = env.action_space\n",
    "        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_rate = 0.99\n",
    "        self.exploration_rate = 1\n",
    "        self.max_exploration = 1\n",
    "        self.min_exploration = 0.01\n",
    "        self.exploration_decay_rate = 0.001\n",
    "\n",
    "    def get_action(self, state):\n",
    "        greedy = random.uniform(0, 1)\n",
    "        if greedy > self.exploration_rate:\n",
    "            action = np.argmax(self.q_table[state, :])\n",
    "        else:\n",
    "            action = self.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, new_state):\n",
    "        self.q_table[state, action] = (1 - self.learning_rate) * self.q_table[state, action] + \\\n",
    "        (self.learning_rate * (reward + (self.discount_rate * np.max(self.q_table[new_state, :]))))\n",
    "        return 1\n",
    "    \n",
    "    def decay_exploration_rate(self, episode):\n",
    "        self.exploration_rate = self.min_exploration + (self.max_exploration - self.min_exploration) * np.exp(-self.exploration_decay_rate * episode)\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884aa85",
   "metadata": {},
   "source": [
    "### Game training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16248a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1999\r"
     ]
    }
   ],
   "source": [
    "agent = Agent(env)\n",
    "rewards = []\n",
    "num_episode = 2000\n",
    "max_step = 100\n",
    "for episode in range(num_episode):\n",
    "    state = env.reset()\n",
    "    reward_episode = 0\n",
    "    print(\"Episode: \"+str(episode), end=\"\\r\")\n",
    "    for s in range(max_step):\n",
    "        #print(env.render('ansi'), end=\"\\r\")\n",
    "        action = agent.get_action(state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        agent.update_q_table(state, action, reward, new_state)\n",
    "        state = new_state\n",
    "        reward_episode += reward\n",
    "        if done:\n",
    "            break\n",
    "    agent.decay_exploration_rate(episode)\n",
    "    rewards.append(reward_episode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5e7363",
   "metadata": {},
   "source": [
    "### Game score output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5096a6a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Average rewards per 1000 episodes ***\n",
      "1000 :  -249.15699999999993\n",
      "2000 :  -35.93700000000005\n",
      "*** Q Table ***\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-3.03170889 -3.52778272 -3.49410222 -3.06755257  8.29507166 -9.77760675]\n",
      " [-1.53780801  0.2198823  -2.14647181  0.26883988 14.01316291 -7.25496642]\n",
      " ...\n",
      " [-1.28957559  5.66442788 -1.51871089 -1.43433587 -3.64407635 -6.84049641]\n",
      " [-3.20816218 -2.44082678 -3.27083891 -3.22766279 -9.13277614 -8.15572745]\n",
      " [-0.3439      0.4767614   0.92892953 16.12821544 -1.899208   -1.73526596]]\n"
     ]
    }
   ],
   "source": [
    "reward_per_thousand_episode = np.split(np.array(rewards), num_episode / 1000)\n",
    "count = 1000\n",
    "print('*** Average rewards per 1000 episodes ***')\n",
    "for r in reward_per_thousand_episode:\n",
    "    print(count, ': ', str(sum(r/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "    \n",
    "print('*** Q Table ***')\n",
    "print(agent.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc145ab",
   "metadata": {},
   "source": [
    "### IA playing game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8805d694",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Game 5 ***\n",
      " +---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def _render_text(env):\n",
    "    desc = env.desc.tolist()\n",
    "    outfile = StringIO()\n",
    "\n",
    "    row, col = env.s // env.ncol, env.s % env.ncol\n",
    "    desc = [[c.decode(\"utf-8\") for c in line] for line in desc]\n",
    "    desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "    for i in range(len(desc)):\n",
    "        for j in range(len(desc[i])):\n",
    "            if desc[i][j] == 'H':\n",
    "                desc[i][j] = utils.colorize(desc[i][j], \"yellow\", highlight=True)\n",
    "            if desc[i][j] == 'F' or desc[i][j] == 'S':\n",
    "                desc[i][j] = utils.colorize(desc[i][j], \"blue\", highlight=True)\n",
    "            if desc[i][j] == 'G':\n",
    "                desc[i][j] = utils.colorize(desc[i][j], \"green\", highlight=True)\n",
    "                \n",
    "    if env.lastaction is not None:\n",
    "        outfile.write(f\"  ({['Left', 'Down', 'Right', 'Up'][env.lastaction]})\\n\")\n",
    "    else:\n",
    "        outfile.write(\"\\n\")\n",
    "    outfile.write(\"\\n\".join(\"\".join(line) for line in desc) + \"\\n\")\n",
    "    return outfile.getvalue()\n",
    "       \n",
    "for episode in range(1):\n",
    "    state = env.reset()\n",
    "    #game = \"*** Game {0} ***\\n\".format(episode + 1)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    for step in range(max_step):\n",
    "        clear_output(wait=True)\n",
    "        taxi_row, taxi_col, pass_idx, dest_idx = env.decode(env.s)\n",
    "        print(game, env.render('human'), end='\\n')\n",
    "        print(pass_idx, dest_idx)\n",
    "        taxi_row, taxi_col, pass_idx, dest_idx = env.decode(env.s)\n",
    "        rend = env.desc.copy()\n",
    "        if pass_idx < 4:\n",
    "            rend[1 + env.locs[pass_idx][0]][2 * env.locs[pass_idx][1] + 1] = 'P'\n",
    "        rend[1 + env.locs[dest_idx][0]][2 * env.locs[dest_idx][1] + 1] = 'D'\n",
    "        rend[1 + taxi_row][2 * taxi_col + 1] = 'T'\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            print(game, env.render('ansi'), end='\\n')\n",
    "            if reward == 1:\n",
    "                #print(\"*** You Won {0} ***\".format(episode_reward))\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                #print(\"*** Your score {0} ***\".format(episode_reward))\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3540d",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a87083",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bc1ba558b89b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnb_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "total_steps, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "max_steps = 100\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    nb_steps, penalties, reward = 0, 0, 0\n",
    "    for step in range(max_steps):\n",
    "        action = agent.get_action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        if done == True:\n",
    "            break\n",
    "        if step >= max_steps:\n",
    "            penalties += 1\n",
    "            break\n",
    "    total_penalties += penalties\n",
    "    total_steps += step\n",
    "        \n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_steps / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c22b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
